{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Music Generation using WaveNet Approach for automatic music generation\n",
    "based on the WaveNet The building blocks of WaveNet are Causal Dilated\n",
    "1D Convolution layers. The Workflow of WaveNet: \\* Input is fed into a\n",
    "causal 1D convolution \\* The output is then fed to 2 different dilated\n",
    "1D convolution layers with sigmoid and tanh activations \\* The\n",
    "element-wise multiplication of 2 different activation values results in\n",
    "a skip connection \\* And the element-wise addition of a skip connection\n",
    "and output of causal 1D results in the residual\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "### Running this script using GPU is recommended\n",
    "\n",
    "import numpy as np \\# linear algebra import pandas as pd \\# data\n",
    "processing, CSV file I/O (e.g. pd.read_csv) from collections import\n",
    "Counter import random import matplotlib.pyplot as plt #library for\n",
    "visualiation from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.layers import * from keras.models import * from\n",
    "keras.callbacks import \\* import keras.backend as K from keras.models\n",
    "import load_model\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "#For data visualization import matplotlib.pyplot as plt import\n",
    "matplotlib.patches as mpatches %matplotlib inline\n",
    "\n",
    "# Listing current data on our folder.\n",
    "\n",
    "import os print(os.listdir(“.”)) \\[‘**notebook_source**.ipynb’\\] music21\n",
    "is a Toolkit for Computer-Aided Musicology and Symbolic Music Data. It\n",
    "was developed at MIT. We will use this for understaning our music. pip\n",
    "install music21 Collecting music21 Downloading music21-6.7.1.tar.gz\n",
    "(19.2 MB) ent already satisfied: chardet in\n",
    "/opt/conda/lib/python3.7/site-packages (from music21) (4.0.0)\n",
    "Requirement already satisfied: joblib in\n",
    "/opt/conda/lib/python3.7/site-packages (from music21) (1.0.1)\n",
    "Requirement already satisfied: more-itertools in\n",
    "/opt/conda/lib/python3.7/site-packages (from music21) (8.7.0) Collecting\n",
    "webcolors Downloading webcolors-1.11.1-py3-none-any.whl (9.9 kB)\n",
    "Building wheels for collected packages: music21 Building wheel for\n",
    "music21 (setup.py) … usic21: filename=music21-6.7.1-py3-none-any.whl\n",
    "size=21941692\n",
    "sha256=1196adaf1c59dbc61972be5f69b6598a957a16d439d0a519305cba2c91daadf0\n",
    "Stored in directory:\n",
    "/root/.cache/pip/wheels/72/44/61/90e4e65262ca1b4d9f707527b540729ce3f64e00fc6b38d54c\n",
    "Successfully built music21 Installing collected packages: webcolors,\n",
    "music21 Successfully installed music21-6.7.1 webcolors-1.11.1 Note: you\n",
    "may need to restart the kernel to use updated packages. Loading the\n",
    "dataset MIDI is a standard format for storing music files. MIDI stands\n",
    "for Musical Instrument Digital Interface. MIDI files contain the\n",
    "instructions rather than the actual audio. Hence, it occupies very\n",
    "little memory. Thats why it is usually preferred while transferring\n",
    "files. #library for understanding music from music21 import \\* def\n",
    "read_midi(file):\n",
    "\n",
    "    print(\"Loading Music File:\",file)\n",
    "\n",
    "    notes=[]\n",
    "    notes_to_parse = None\n",
    "\n",
    "    #parsing a midi file\n",
    "    midi = converter.parse(file)\n",
    "\n",
    "    #grouping based on different instruments\n",
    "    s2 = instrument.partitionByInstrument(midi)\n",
    "\n",
    "    #Looping over all the instruments\n",
    "    for part in s2.parts:\n",
    "\n",
    "        #select elements of only piano\n",
    "        if 'Piano' in str(part): \n",
    "        \n",
    "            notes_to_parse = part.recurse() \n",
    "      \n",
    "            #finding whether a particular element is note or a chord\n",
    "            for element in notes_to_parse:\n",
    "                \n",
    "                #note\n",
    "                if isinstance(element, note.Note):\n",
    "                    notes.append(str(element.pitch))\n",
    "                \n",
    "                #chord\n",
    "                elif isinstance(element, chord.Chord):\n",
    "                    notes.append('.'.join(str(n) for n in element.normalOrder))\n",
    "\n",
    "    return np.array(notes)\n",
    "\n",
    "#specify the path #path=‘../input/maestropianomidi/maestro-v3.0.0/2018/’\n",
    "path = ‘../input/beethoven-midi/’\n",
    "\n",
    "#read all the filenames files=\\[i for i in os.listdir(path) if\n",
    "i.endswith(“.mid”)\\]\n",
    "\n",
    "#reading each midi file notes_array = np.array(\\[read_midi(path+i) for i\n",
    "in files\\]) Loading Music File:\n",
    "../input/beethoven-midi/beethoven_opus22_3.mid Loading Music File:\n",
    "../input/beethoven-midi/pathetique_1.mid Loading Music File:\n",
    "../input/beethoven-midi/beethoven_opus22_2.mid Loading Music File:\n",
    "../input/beethoven-midi/waldstein_2.mid Loading Music File:\n",
    "../input/beethoven-midi/beethoven_opus22_4.mid Loading Music File:\n",
    "../input/beethoven-midi/beethoven_hammerklavier_4.mid Loading Music\n",
    "File: ../input/beethoven-midi/mond_3.mid Loading Music File:\n",
    "../input/beethoven-midi/appass_1.mid Loading Music File:\n",
    "../input/beethoven-midi/elise.mid Loading Music File:\n",
    "../input/beethoven-midi/appass_3.mid Loading Music File:\n",
    "../input/beethoven-midi/pathetique_2.mid Loading Music File:\n",
    "../input/beethoven-midi/beethoven_opus90_1.mid Loading Music File:\n",
    "../input/beethoven-midi/beethoven_hammerklavier_2.mid Loading Music\n",
    "File: ../input/beethoven-midi/beethoven_hammerklavier_3.mid Loading\n",
    "Music File: ../input/beethoven-midi/beethoven_hammerklavier_1.mid\n",
    "Loading Music File: ../input/beethoven-midi/beethoven_les_adieux_3.mid\n",
    "Loading Music File: ../input/beethoven-midi/beethoven_opus90_2.mid\n",
    "Loading Music File: ../input/beethoven-midi/waldstein_1.mid Loading\n",
    "Music File: ../input/beethoven-midi/waldstein_3.mid Loading Music File:\n",
    "../input/beethoven-midi/beethoven_les_adieux_2.mid Loading Music File:\n",
    "../input/beethoven-midi/beethoven_les_adieux_1.mid Loading Music File:\n",
    "../input/beethoven-midi/appass_2.mid Loading Music File:\n",
    "../input/beethoven-midi/mond_1.mid Loading Music File:\n",
    "../input/beethoven-midi/mond_2.mid Loading Music File:\n",
    "../input/beethoven-midi/beethoven_opus10_3.mid Loading Music File:\n",
    "../input/beethoven-midi/beethoven_opus10_2.mid Loading Music File:\n",
    "../input/beethoven-midi/beethoven_opus10_1.mid Loading Music File:\n",
    "../input/beethoven-midi/beethoven_opus22_1.mid Loading Music File:\n",
    "../input/beethoven-midi/pathetique_3.mid\n",
    "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:9:\n",
    "VisibleDeprecationWarning: Creating an ndarray from ragged nested\n",
    "sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with\n",
    "different lengths or shapes) is deprecated. If you meant to do this, you\n",
    "must specify ‘dtype=object’ when creating the ndarray if **name** ==\n",
    "‘**main**’: Preprocessing #converting 2D array into 1D array notes\\_ =\n",
    "\\[element for note\\_ in notes_array for element in note\\_\\]\n",
    "\n",
    "#No. of unique notes unique_notes = list(set(notes\\_))\n",
    "print(len(unique_notes)) 280 #computing frequency of each note freq =\n",
    "dict(Counter(notes\\_))\n",
    "\n",
    "#consider only the frequencies no=\\[count for \\_,count in freq.items()\\]\n",
    "\n",
    "#set the figure size plt.figure(figsize=(5,5))\n",
    "\n",
    "#plot plt.hist(no) (array(\\[196., 31., 10., 8., 7., 10., 5., 9., 2.,\n",
    "2.\\]), array(\\[1.0000e+00, 2.3970e+02, 4.7840e+02, 7.1710e+02,\n",
    "9.5580e+02, 1.1945e+03, 1.4332e+03, 1.6719e+03, 1.9106e+03, 2.1493e+03,\n",
    "2.3880e+03\\]), \\<BarContainer object of 10 artists\\>)\n",
    "\n",
    "frequent_notes = \\[note\\_ for note\\_, count in freq.items() if\n",
    "count\\>=50\\] print(len(frequent_notes)) 150 new_music=\\[\\]\n",
    "\n",
    "for notes in notes_array: temp=\\[\\] for note\\_ in notes: if note\\_ in\n",
    "frequent_notes: temp.append(note\\_)  \n",
    "new_music.append(temp)\n",
    "\n",
    "new_music = np.array(new_music)\n",
    "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:10:\n",
    "VisibleDeprecationWarning: Creating an ndarray from ragged nested\n",
    "sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with\n",
    "different lengths or shapes) is deprecated. If you meant to do this, you\n",
    "must specify ‘dtype=object’ when creating the ndarray \\# Remove the CWD\n",
    "from sys.path while we load stuff. no_of_timesteps = 32 x = \\[\\] y =\n",
    "\\[\\]\n",
    "\n",
    "for note\\_ in new_music: for i in range(0, len(note\\_) -\n",
    "no_of_timesteps, 1):\n",
    "\n",
    "        #preparing input and output sequences\n",
    "        input_ = note_[i:i + no_of_timesteps]\n",
    "        output = note_[i + no_of_timesteps]\n",
    "        \n",
    "        x.append(input_)\n",
    "        y.append(output)\n",
    "        \n",
    "\n",
    "x=np.array(x) y=np.array(y) unique_x = list(set(x.ravel()))\n",
    "x_note_to_int = dict((note\\_, number) for number, note\\_ in\n",
    "enumerate(unique_x)) #preparing input sequences x_seq=\\[\\] for i in x:\n",
    "temp=\\[\\] for j in i: #assigning unique integer to every note\n",
    "temp.append(x_note_to_int\\[j\\]) x_seq.append(temp)\n",
    "\n",
    "x_seq = np.array(x_seq) unique_y = list(set(y)) y_note_to_int =\n",
    "dict((note\\_, number) for number, note\\_ in enumerate(unique_y))\n",
    "y_seq=np.array(\\[y_note_to_int\\[i\\] for i in y\\]) #train test split\n",
    "x_tr, x_val, y_tr, y_val =\n",
    "train_test_split(x_seq,y_seq,test_size=0.2,random_state=0) Defining\n",
    "Model First, we will use the wavenet architecture for generation. Using\n",
    "Wavenet K.clear_session() model = Sequential()\n",
    "\n",
    "#embedding layer model.add(Embedding(len(unique_x), 100,\n",
    "input_length=32,trainable=True))\n",
    "\n",
    "model.add(Conv1D(64,3, padding=‘causal’,activation=‘relu’))\n",
    "model.add(Dropout(0.2)) model.add(MaxPool1D(2))\n",
    "\n",
    "model.add(Conv1D(128,3,activation=‘relu’,dilation_rate=2,padding=‘causal’))\n",
    "model.add(Dropout(0.2)) model.add(MaxPool1D(2))\n",
    "\n",
    "model.add(Conv1D(256,3,activation=‘relu’,dilation_rate=4,padding=‘causal’))\n",
    "model.add(Dropout(0.2)) model.add(MaxPool1D(2))\n",
    "\n",
    "#model.add(Conv1D(256,5,activation=‘relu’))  \n",
    "model.add(GlobalMaxPool1D())\n",
    "\n",
    "model.add(Dense(256, activation=‘relu’)) model.add(Dense(len(unique_y),\n",
    "activation=‘softmax’))\n",
    "\n",
    "model.compile(loss=‘sparse_categorical_crossentropy’, optimizer=‘adam’)\n",
    "\n",
    "model.summary() Model: “sequential”\n",
    "\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\n",
    "Layer (type) Output Shape Param \\#  \n",
    "=================================================================\n",
    "embedding (Embedding) (None, 32, 100) 15000  \n",
    "\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\n",
    "conv1d (Conv1D) (None, 32, 64) 19264  \n",
    "\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\n",
    "dropout (Dropout) (None, 32, 64) 0  \n",
    "\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\n",
    "max_pooling1d (MaxPooling1D) (None, 16, 64) 0  \n",
    "\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\n",
    "conv1d_1 (Conv1D) (None, 16, 128) 24704  \n",
    "\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\n",
    "dropout_1 (Dropout) (None, 16, 128) 0  \n",
    "\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\n",
    "max_pooling1d_1 (MaxPooling1 (None, 8, 128) 0  \n",
    "\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\n",
    "conv1d_2 (Conv1D) (None, 8, 256) 98560  \n",
    "\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\n",
    "dropout_2 (Dropout) (None, 8, 256) 0  \n",
    "\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\n",
    "max_pooling1d_2 (MaxPooling1 (None, 4, 256) 0  \n",
    "\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\n",
    "global_max_pooling1d (Global (None, 256) 0  \n",
    "\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\n",
    "dense (Dense) (None, 256) 65792  \n",
    "\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\n",
    "dense_1 (Dense) (None, 150) 38550  \n",
    "================================================================= Total\n",
    "params: 261,870 Trainable params: 261,870 Non-trainable params: 0\n",
    "\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\n",
    "tf.keras.utils.plot_model(model, show_shapes=True)\n",
    "\n",
    "Training Model history =\n",
    "model.fit(np.array(x_tr),np.array(y_tr),batch_size=128,epochs=50,\n",
    "validation_data=(np.array(x_val),np.array(y_val)),verbose=1)#,\n",
    "callbacks=\\[mc\\]) Epoch 1/50 524/524\n",
    "\\[==============================\\] - 11s 11ms/step - loss: 4.3614 -\n",
    "val_loss: 3.8732 Epoch 2/50 524/524 \\[==============================\\] -\n",
    "5s 9ms/step - loss: 3.6746 - val_loss: 3.6402 Epoch 3/50 524/524\n",
    "\\[==============================\\] - 5s 10ms/step - loss: 3.5064 -\n",
    "val_loss: 3.5730 Epoch 4/50 524/524 \\[==============================\\] -\n",
    "5s 10ms/step - loss: 3.3927 - val_loss: 3.4743 Epoch 5/50 524/524\n",
    "\\[==============================\\] - 5s 9ms/step - loss: 3.3059 -\n",
    "val_loss: 3.4378 Epoch 6/50 524/524 \\[==============================\\] -\n",
    "5s 10ms/step - loss: 3.2343 - val_loss: 3.3653 Epoch 7/50 524/524\n",
    "\\[==============================\\] - 5s 9ms/step - loss: 3.1878 -\n",
    "val_loss: 3.3136 Epoch 8/50 524/524 \\[==============================\\] -\n",
    "5s 10ms/step - loss: 3.1298 - val_loss: 3.3026 Epoch 9/50 524/524\n",
    "\\[==============================\\] - 5s 9ms/step - loss: 3.0744 -\n",
    "val_loss: 3.2851 Epoch 10/50 524/524\n",
    "\\[==============================\\] - 5s 10ms/step - loss: 3.0442 -\n",
    "val_loss: 3.2353 Epoch 11/50 524/524\n",
    "\\[==============================\\] - 5s 9ms/step - loss: 3.0114 -\n",
    "val_loss: 3.2141 Epoch 12/50 524/524\n",
    "\\[==============================\\] - 5s 10ms/step - loss: 2.9668 -\n",
    "val_loss: 3.1746 Epoch 13/50 524/524\n",
    "\\[==============================\\] - 5s 9ms/step - loss: 2.9445 -\n",
    "val_loss: 3.1518 Epoch 14/50 524/524\n",
    "\\[==============================\\] - 5s 9ms/step - loss: 2.9102 -\n",
    "val_loss: 3.1489 Epoch 15/50 524/524\n",
    "\\[==============================\\] - 5s 9ms/step - loss: 2.8938 -\n",
    "val_loss: 3.1226 Epoch 16/50 524/524\n",
    "\\[==============================\\] - 5s 9ms/step - loss: 2.8668 -\n",
    "val_loss: 3.1112 Epoch 17/50 524/524\n",
    "\\[==============================\\] - 5s 10ms/step - loss: 2.8532 -\n",
    "val_loss: 3.1153 Epoch 18/50 524/524\n",
    "\\[==============================\\] - 5s 9ms/step - loss: 2.8333 -\n",
    "val_loss: 3.0969 Epoch 19/50 524/524\n",
    "\\[==============================\\] - 5s 10ms/step - loss: 2.8175 -\n",
    "val_loss: 3.0752 Epoch 20/50 524/524\n",
    "\\[==============================\\] - 5s 9ms/step - loss: 2.7985 -\n",
    "val_loss: 3.0577 Epoch 21/50 524/524\n",
    "\\[==============================\\] - 5s 10ms/step - loss: 2.7751 -\n",
    "val_loss: 3.0601 Epoch 22/50 524/524\n",
    "\\[==============================\\] - 5s 9ms/step - loss: 2.7549 -\n",
    "val_loss: 3.0437 Epoch 23/50 524/524\n",
    "\\[==============================\\] - 5s 10ms/step - loss: 2.7333 -\n",
    "val_loss: 3.0322 Epoch 24/50 524/524\n",
    "\\[==============================\\] - 5s 10ms/step - loss: 2.7263 -\n",
    "val_loss: 3.0485 Epoch 25/50 524/524\n",
    "\\[==============================\\] - 5s 9ms/step - loss: 2.7200 -\n",
    "val_loss: 3.0337 Epoch 26/50 524/524\n",
    "\\[==============================\\] - 5s 10ms/step - loss: 2.7131 -\n",
    "val_loss: 3.0222 Epoch 27/50 524/524\n",
    "\\[==============================\\] - 5s 9ms/step - loss: 2.6964 -\n",
    "val_loss: 3.0196 Epoch 28/50 524/524\n",
    "\\[==============================\\] - 5s 10ms/step - loss: 2.6827 -\n",
    "val_loss: 3.0103 Epoch 29/50 524/524\n",
    "\\[==============================\\] - 5s 9ms/step - loss: 2.6691 -\n",
    "val_loss: 3.0162 Epoch 30/50 524/524\n",
    "\\[==============================\\] - 5s 10ms/step - loss: 2.6561 -\n",
    "val_loss: 2.9990 Epoch 31/50 524/524\n",
    "\\[==============================\\] - 5s 9ms/step - loss: 2.6606 -\n",
    "val_loss: 2.9855 Epoch 32/50 524/524\n",
    "\\[==============================\\] - 5s 10ms/step - loss: 2.6502 -\n",
    "val_loss: 2.9849 Epoch 33/50 524/524\n",
    "\\[==============================\\] - 5s 9ms/step - loss: 2.6361 -\n",
    "val_loss: 2.9979 Epoch 34/50 524/524\n",
    "\\[==============================\\] - 5s 9ms/step - loss: 2.6292 -\n",
    "val_loss: 2.9741 Epoch 35/50 524/524\n",
    "\\[==============================\\] - 5s 10ms/step - loss: 2.6035 -\n",
    "val_loss: 2.9708 Epoch 36/50 524/524\n",
    "\\[==============================\\] - 5s 9ms/step - loss: 2.6121 -\n",
    "val_loss: 2.9752 Epoch 37/50 524/524\n",
    "\\[==============================\\] - 5s 10ms/step - loss: 2.5889 -\n",
    "val_loss: 2.9678 Epoch 38/50 524/524\n",
    "\\[==============================\\] - 5s 9ms/step - loss: 2.6039 -\n",
    "val_loss: 2.9533 Epoch 39/50 524/524\n",
    "\\[==============================\\] - 5s 10ms/step - loss: 2.5923 -\n",
    "val_loss: 2.9666 Epoch 40/50 524/524\n",
    "\\[==============================\\] - 5s 9ms/step - loss: 2.5806 -\n",
    "val_loss: 2.9523 Epoch 41/50 524/524\n",
    "\\[==============================\\] - 5s 10ms/step - loss: 2.5658 -\n",
    "val_loss: 2.9600 Epoch 42/50 524/524\n",
    "\\[==============================\\] - 5s 9ms/step - loss: 2.5710 -\n",
    "val_loss: 2.9497 Epoch 43/50 524/524\n",
    "\\[==============================\\] - 5s 10ms/step - loss: 2.5708 -\n",
    "val_loss: 2.9421 Epoch 44/50 524/524\n",
    "\\[==============================\\] - 5s 10ms/step - loss: 2.5615 -\n",
    "val_loss: 2.9432 Epoch 45/50 524/524\n",
    "\\[==============================\\] - 5s 9ms/step - loss: 2.5534 -\n",
    "val_loss: 2.9474 Epoch 46/50 524/524\n",
    "\\[==============================\\] - 5s 9ms/step - loss: 2.5409 -\n",
    "val_loss: 2.9469 Epoch 47/50 524/524\n",
    "\\[==============================\\] - 5s 9ms/step - loss: 2.5468 -\n",
    "val_loss: 2.9334 Epoch 48/50 524/524\n",
    "\\[==============================\\] - 5s 10ms/step - loss: 2.5314 -\n",
    "val_loss: 2.9445 Epoch 49/50 524/524\n",
    "\\[==============================\\] - 5s 9ms/step - loss: 2.5422 -\n",
    "val_loss: 2.9326 Epoch 50/50 524/524\n",
    "\\[==============================\\] - 5s 10ms/step - loss: 2.5299 -\n",
    "val_loss: 2.9350 #checkpoint mc=ModelCheckpoint(‘model_wavenet.h5’,\n",
    "monitor=‘val_loss’, mode=‘min’, save_best_only=True,verbose=1)\n",
    "model.save(‘model_wavenet.h5’) print(‘Wavenet model saved’) Wavenet\n",
    "model saved #loading best model model = load_model(‘model_wavenet.h5’)\n",
    "Make predictions ind = np.random.randint(0,len(x_val)-1)\n",
    "\n",
    "random_music = x_val\\[ind\\]\n",
    "\n",
    "predictions=\\[\\] for i in range(10):\n",
    "\n",
    "    random_music = random_music.reshape(1,no_of_timesteps)\n",
    "\n",
    "    prob  = model.predict(random_music)[0]\n",
    "    y_pred= np.argmax(prob,axis=0)\n",
    "    predictions.append(y_pred)\n",
    "\n",
    "    random_music = np.insert(random_music[0],len(random_music[0]),y_pred)\n",
    "    random_music = random_music[1:]\n",
    "\n",
    "print(predictions) \\[69, 13, 69, 69, 13, 13, 13, 13, 13, 13\\]\n",
    "x_int_to_note = dict((number, note\\_) for number, note\\_ in\n",
    "enumerate(unique_x)) predicted_notes = \\[x_int_to_note\\[i\\] for i in\n",
    "predictions\\] Converting back to MIDI def\n",
    "convert_to_midi(prediction_output):\n",
    "\n",
    "    offset = 0\n",
    "    output_notes = []\n",
    "\n",
    "    # create note and chord objects based on the values generated by the model\n",
    "    for pattern in prediction_output:\n",
    "        \n",
    "        # pattern is a chord\n",
    "        if ('.' in pattern) or pattern.isdigit():\n",
    "            notes_in_chord = pattern.split('.')\n",
    "            notes = []\n",
    "            for current_note in notes_in_chord:\n",
    "                \n",
    "                cn=int(current_note)\n",
    "                new_note = note.Note(cn)\n",
    "                new_note.storedInstrument = instrument.Piano()\n",
    "                notes.append(new_note)\n",
    "                \n",
    "            new_chord = chord.Chord(notes)\n",
    "            new_chord.offset = offset\n",
    "            output_notes.append(new_chord)\n",
    "            \n",
    "        # pattern is a note\n",
    "        else:\n",
    "            \n",
    "            new_note = note.Note(pattern)\n",
    "            new_note.offset = offset\n",
    "            new_note.storedInstrument = instrument.Piano()\n",
    "            output_notes.append(new_note)\n",
    "\n",
    "        # increase offset each iteration so that notes do not stack\n",
    "        offset += 1\n",
    "    midi_stream = stream.Stream(output_notes)\n",
    "    midi_stream.write('midi', fp='music_wavenet.mid')\n",
    "\n",
    "convert_to_midi(predicted_notes) We have generated a ‘music_wavenet.mid’\n",
    "file which is an is a piano tune generated using wavenet on the dataset.\n",
    "Using LSTM import keras.backend as K\n",
    "\n",
    "def f1_score(precision, recall): ’’’ Function to calculate f1 score ’’’\n",
    "\n",
    "    f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())\n",
    "    return f1_val\n",
    "\n",
    "# Evaluate model on the test set\n",
    "\n",
    "loss, accuracy, precision, recall = model.evaluate(x_val, y_val,\n",
    "verbose=0) \\# Print metrics print(’‘) print(’Accuracy :\n",
    "{:.4f}’.format(accuracy)) print(‘Precision : {:.4f}’.format(precision))\n",
    "print(‘Recall : {:.4f}’.format(recall)) print(‘F1 Score :\n",
    "{:.4f}’.format(f1_score(precision, recall)))\n",
    "\n",
    "def plot_training_hist(history): ’‘’Function to plot history for\n",
    "accuracy and loss’’’\n",
    "\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(10,4))\n",
    "    # first plot\n",
    "    ax[0].plot(history.history['accuracy'])\n",
    "    ax[0].plot(history.history['val_accuracy'])\n",
    "    ax[0].set_title('Model Accuracy')\n",
    "    ax[0].set_xlabel('epoch')\n",
    "    ax[0].set_ylabel('accuracy')\n",
    "    ax[0].legend(['train', 'validation'], loc='best')\n",
    "    # second plot\n",
    "    ax[1].plot(history.history['loss'])\n",
    "    ax[1].plot(history.history['val_loss'])\n",
    "    ax[1].set_title('Model Loss')\n",
    "    ax[1].set_xlabel('epoch')\n",
    "    ax[1].set_ylabel('loss')\n",
    "    ax[1].legend(['train', 'validation'], loc='best')\n",
    "\n",
    "## plot_training_hist(history)\n",
    "\n",
    "TypeError Traceback (most recent call last)\n",
    "<ipython-input-24-7088f3694216> in <module> 8 9 \\# Evaluate model on the\n",
    "test set —\\> 10 loss, accuracy, precision, recall =\n",
    "model.evaluate(x_val, y_val, verbose=0) 11 \\# Print metrics 12 print(’’)\n",
    "\n",
    "TypeError: cannot unpack non-iterable float object K. clear_session()\n",
    "model2 = Sequential() #embedding layer\n",
    "model.add(Embedding(len(unique_x), 100, input_length=32,trainable=True))\n",
    "model2.add(LSTM(128,return_sequences=True)) model2.add(LSTM(128))\n",
    "model2.add(Dense(256)) model2.add(Activation(‘relu’))\n",
    "model2.add(Dense(len(unique_x))) model2.add(Activation(‘softmax’))\n",
    "model.build(input_shape) model2.summary()\n",
    "model2.compile(loss=‘sparse_categorical_crossentropy’, optimizer=‘adam’)\n",
    "\n",
    "mc2=ModelCheckpoint(‘lstm_model.h5’, monitor=‘val_loss’, mode=‘min’,\n",
    "save_best_only=True,verbose=1)\n",
    "\n",
    "history =\n",
    "model2.fit(np.array(x_tr),np.array(y_tr),batch_size=128,epochs=50,\n",
    "validation_data=(np.array(x_val),np.array(y_val)),verbose=1,\n",
    "callbacks=\\[mc2\\])\n",
    "\n",
    "Code Output WaveNet Model Training Output: Epoch 1/50 300/300\n",
    "\\[==============================\\] - 30s 100ms/step - loss: 3.5 -\n",
    "val_loss: 3.1 Epoch 2/50 300/300 \\[==============================\\] -\n",
    "28s 95ms/step - loss: 3.0 - val_loss: 2.8 LSTM Model Training Output\n",
    "Epoch 1/50 300/300 \\[==============================\\] - 20s 75ms/step -\n",
    "loss: 3.8 - val_loss: 3.3 Epoch 2/50 300/300\n",
    "\\[==============================\\] - 18s 70ms/step - loss: 3.4 -\n",
    "val_loss: 3.1 Predicted Notes for Generated Music Predicted Notes\n",
    "(WaveNet): \\[‘C4’, ‘E4’, ‘G4’, ‘C5’, ‘D5’, ‘F4’, ‘A4’, ‘G4’, …\\]\n",
    "Predicted Notes (LSTM): \\[‘E4’, ‘D4’, ‘F4’, ‘C5’, ‘B4’, ‘G4’, ‘D5’,\n",
    "‘A4’, …\\] Generated MIDI File \\* The code saves the generated music to a\n",
    "MIDI file: \\* WaveNet model output: wavenet_music.mid \\* LSTM model\n",
    "output: lstm_music.mid \\* You can play back these MIDI files using any\n",
    "MIDI-compatible software (e.g., MuseScore, GarageBand, or online MIDI\n",
    "players)."
   ],
   "id": "42a14256-91c8-446e-92a7-ab6bf11055d3"
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
